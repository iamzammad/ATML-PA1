{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9503961,"sourceType":"datasetVersion","datasetId":5784356},{"sourceId":9508569,"sourceType":"datasetVersion","datasetId":5787586},{"sourceId":9508925,"sourceType":"datasetVersion","datasetId":5787853}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Getting the CLIP Model from GitHub","metadata":{}},{"cell_type":"code","source":"!conda install --yes -c pytorch pytorch=1.7.1 torchvision cudatoolkit=11.0\n!pip install ftfy regex tqdm\n!pip install git+https://github.com/openai/CLIP.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-29T09:52:39.201670Z","iopub.execute_input":"2024-09-29T09:52:39.202015Z","iopub.status.idle":"2024-09-29T10:17:27.170775Z","shell.execute_reply.started":"2024-09-29T09:52:39.201977Z","shell.execute_reply":"2024-09-29T10:17:27.169675Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Retrieving notices: ...working... done\nChannels:\n - pytorch\n - rapidsai\n - nvidia\n - nodefaults\n - conda-forge\n - defaults\nPlatform: linux-64\nCollecting package metadata (repodata.json): done\nSolving environment: | \u001b[33m\u001b[1mwarning  libmamba\u001b[m Added empty dependency for problem type SOLVER_RULE_UPDATE\nfailed\n\nLibMambaUnsatisfiableError: Encountered problems while solving:\n  - package pytorch-1.7.1-py3.6_cpu_0 requires python >=3.6,<3.7.0a0, but none of the providers can be installed\n  - package cuda-version-12.3-h32bc705_3 has constraint cudatoolkit 12.3|12.3.* conflicting with cudatoolkit-11.0.221-h6bb024c_0\n\nCould not solve for environment specs\nThe following packages are incompatible\n├─ \u001b[32mcuda-version 12.3** \u001b[0m is installable and it requires\n│  └─ \u001b[32mcudatoolkit 12.3|12.3.* \u001b[0m, which can be installed;\n├─ \u001b[31mcudatoolkit 11.0** \u001b[0m is not installable because it conflicts with any installable versions previously reported;\n├─ \u001b[32mpin-1\u001b[0m is installable and it requires\n│  └─ \u001b[32mpython 3.10.* \u001b[0m, which can be installed;\n└─ \u001b[31mpytorch 1.7.1** \u001b[0m is not installable because there are no viable options\n   ├─ \u001b[31mpytorch 1.7.1\u001b[0m would require\n   │  └─ \u001b[31mpython >=3.6,<3.7.0a0 \u001b[0m, which conflicts with any installable versions previously reported;\n   ├─ \u001b[31mpytorch 1.7.1\u001b[0m would require\n   │  └─ \u001b[31mpython >=3.7,<3.8.0a0 \u001b[0m, which conflicts with any installable versions previously reported;\n   ├─ \u001b[31mpytorch 1.7.1\u001b[0m would require\n   │  └─ \u001b[31mpython >=3.8,<3.9.0a0 \u001b[0m, which conflicts with any installable versions previously reported;\n   └─ \u001b[31mpytorch 1.7.1\u001b[0m would require\n      └─ \u001b[31mpython >=3.9,<3.10.0a0 \u001b[0m, which conflicts with any installable versions previously reported.\n\nCollecting ftfy\n  Downloading ftfy-6.2.3-py3-none-any.whl.metadata (7.8 kB)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (2024.5.15)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.4)\nRequirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/conda/lib/python3.10/site-packages (from ftfy) (0.2.13)\nDownloading ftfy-6.2.3-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.0/43.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: ftfy\nSuccessfully installed ftfy-6.2.3\nCollecting git+https://github.com/openai/CLIP.git\n  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-le__wil1\n  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-le__wil1\n  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: ftfy in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (6.2.3)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (21.3)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (2024.5.15)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (4.66.4)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (2.4.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (0.19.0)\nRequirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/conda/lib/python3.10/site-packages (from ftfy->clip==1.0) (0.2.13)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->clip==1.0) (3.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (2024.6.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (10.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->clip==1.0) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->clip==1.0) (1.3.0)\nBuilding wheels for collected packages: clip\n  Building wheel for clip (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=766c3339b40560d41c47d1019dd05afd86dbb69ac6b1232d84163c8e8bccf578\n  Stored in directory: /tmp/pip-ephem-wheel-cache-kktw61z3/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\nSuccessfully built clip\nInstalling collected packages: clip\nSuccessfully installed clip-1.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport zipfile\nimport clip\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader, Dataset\nimport matplotlib.pyplot as plt\nimport pandas as pd  \nfrom tabulate import tabulate ","metadata":{"execution":{"iopub.status.busy":"2024-09-29T12:19:00.967492Z","iopub.execute_input":"2024-09-29T12:19:00.968211Z","iopub.status.idle":"2024-09-29T12:19:00.973260Z","shell.execute_reply.started":"2024-09-29T12:19:00.968174Z","shell.execute_reply":"2024-09-29T12:19:00.972288Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\noriginal_data_dir = '/kaggle/input/animal10-shape/original/val'\ncanny_data_dir = '/kaggle/input/animal10-shape/canny/val'  \n\nclasses = sorted(os.listdir(original_data_dir))","metadata":{"execution":{"iopub.status.busy":"2024-09-29T12:19:13.261329Z","iopub.execute_input":"2024-09-29T12:19:13.262211Z","iopub.status.idle":"2024-09-29T12:19:13.289568Z","shell.execute_reply.started":"2024-09-29T12:19:13.262167Z","shell.execute_reply":"2024-09-29T12:19:13.288560Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"# Custom Class to Read Dataset","metadata":{}},{"cell_type":"code","source":"class CannyImageDataset(Dataset):\n    def __init__(self, root_dir, classes, transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.classes = classes\n        self.image_paths = []\n        self.labels = []\n\n        for class_index, class_name in enumerate(classes):\n            class_folder = os.path.join(root_dir, class_name)\n            for image_name in os.listdir(class_folder):\n                if image_name.endswith(('.png', '.jpg', '.jpeg')):\n                    self.image_paths.append(os.path.join(class_folder, image_name))\n                    self.labels.append(class_index)\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        image = Image.open(img_path).convert('RGB')\n        label = self.labels[idx]\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label, img_path","metadata":{"execution":{"iopub.status.busy":"2024-09-29T12:19:57.173900Z","iopub.execute_input":"2024-09-29T12:19:57.174280Z","iopub.status.idle":"2024-09-29T12:19:57.183350Z","shell.execute_reply.started":"2024-09-29T12:19:57.174244Z","shell.execute_reply":"2024-09-29T12:19:57.182422Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"preprocess = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\noriginal_dataset = CannyImageDataset(root_dir=original_data_dir, classes=classes, transform=preprocess)\ncanny_dataset = CannyImageDataset(root_dir=canny_data_dir, classes=classes, transform=preprocess)\n\noriginal_dataloader = DataLoader(original_dataset, batch_size=32, shuffle=False, num_workers=2)\ncanny_dataloader = DataLoader(canny_dataset, batch_size=32, shuffle=False, num_workers=2)\n\nmodel, _ = clip.load(\"ViT-B/32\", device=device)\n\ntext_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in classes]).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-09-29T12:20:38.071889Z","iopub.execute_input":"2024-09-29T12:20:38.072320Z","iopub.status.idle":"2024-09-29T12:20:42.561450Z","shell.execute_reply.started":"2024-09-29T12:20:38.072282Z","shell.execute_reply":"2024-09-29T12:20:42.560312Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"# Evaluate CLIP on dataset","metadata":{}},{"cell_type":"code","source":"def evaluate(model, dataloader, text_inputs):\n    total = 0\n    correct = 0\n    predictions = []\n    labels = []\n\n    model.eval()\n\n    with torch.no_grad():\n        for images, lbls, img_paths in tqdm(dataloader):\n            images = images.to(device)\n            lbls = lbls.to(device)\n\n            image_features = model.encode_image(images)\n            image_features /= image_features.norm(dim=-1, keepdim=True)\n            text_features = model.encode_text(text_inputs)\n            text_features /= text_features.norm(dim=-1, keepdim=True)\n\n            similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n            _, predicted = similarity.max(dim=1)\n\n            total += lbls.size(0)\n            correct += (predicted == lbls).sum().item()\n            predictions.extend(predicted.cpu().numpy())\n            labels.extend(lbls.cpu().numpy())\n\n    accuracy = correct / total\n    return accuracy, predictions, labels","metadata":{"execution":{"iopub.status.busy":"2024-09-29T12:21:02.655254Z","iopub.execute_input":"2024-09-29T12:21:02.656093Z","iopub.status.idle":"2024-09-29T12:21:02.664397Z","shell.execute_reply.started":"2024-09-29T12:21:02.656055Z","shell.execute_reply":"2024-09-29T12:21:02.663494Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"# Calculate accuracies and shape bias","metadata":{}},{"cell_type":"code","source":"original_accuracy, original_predictions, original_labels = evaluate(model, original_dataloader, text_inputs)\nprint(\"Accuracy on original dataset: \", original_accuracy)\ncanny_accuracy, canny_predictions, canny_labels = evaluate(model, canny_dataloader, text_inputs)\nprint(\"Accuracy on canny dataset: \", canny_accuracy)\nprint(\"Shape bias of the model: \", canny_accuracy/original_accuracy)","metadata":{"execution":{"iopub.status.busy":"2024-09-29T12:23:28.022530Z","iopub.execute_input":"2024-09-29T12:23:28.023259Z","iopub.status.idle":"2024-09-29T12:23:39.048455Z","shell.execute_reply.started":"2024-09-29T12:23:28.023217Z","shell.execute_reply":"2024-09-29T12:23:39.047260Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stderr","text":"100%|██████████| 32/32 [00:05<00:00,  5.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy on original dataset:  0.602\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 32/32 [00:05<00:00,  5.84it/s]","output_type":"stream"},{"name":"stdout","text":"Accuracy on canny dataset:  0.318\nShape bias of the model:  0.5282392026578073\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# See class wise data","metadata":{}},{"cell_type":"code","source":"total_per_class = {class_name: 0 for class_name in classes}\ncorrect_original_incorrect_canny = {class_name: 0 for class_name in classes}\ncorrect_both = {class_name: 0 for class_name in classes}\nincorrect_both = {class_name: 0 for class_name in classes}\ncorrect_canny_incorrect_original = {class_name: 0 for class_name in classes}\n\nfor i in range(len(original_labels)):\n    class_name = classes[original_labels[i]]\n    total_per_class[class_name] += 1\n\n    if original_predictions[i] == original_labels[i] and canny_predictions[i] != original_labels[i]:\n        correct_original_incorrect_canny[class_name] += 1\n    if original_predictions[i] == original_labels[i] and canny_predictions[i] == original_labels[i]:\n        correct_both[class_name] += 1\n    if original_predictions[i] != original_labels[i] and canny_predictions[i] != original_labels[i]:\n        incorrect_both[class_name] += 1\n    if original_predictions[i] != original_labels[i] and canny_predictions[i] == original_labels[i]:\n        correct_canny_incorrect_original[class_name] += 1\n\npercentages = {}\nfor class_name in classes:\n    percentages[class_name] = {\n        \"correct_original_incorrect_canny\": (correct_original_incorrect_canny[class_name] / total_per_class[class_name]) * 100 if total_per_class[class_name] > 0 else 0,\n        \"correct_both\": (correct_both[class_name] / total_per_class[class_name]) * 100 if total_per_class[class_name] > 0 else 0,\n        \"incorrect_both\": (incorrect_both[class_name] / total_per_class[class_name]) * 100 if total_per_class[class_name] > 0 else 0,\n        \"correct_canny_incorrect_original\": (correct_canny_incorrect_original[class_name] / total_per_class[class_name]) * 100 if total_per_class[class_name] > 0 else 0,\n    }\n\npercentage_table = []\nfor class_name, data in percentages.items():\n    percentage_table.append({\n        \"Class\": class_name,\n        \"Correct Original, Incorrect Canny (%)\": round(data[\"correct_original_incorrect_canny\"], 1),\n        \"Correct Both (%)\": round(data[\"correct_both\"], 1),\n        \"Incorrect Both (%)\": round(data[\"incorrect_both\"], 1),\n        \"Correct Canny, Incorrect Original (%)\": round(data[\"correct_canny_incorrect_original\"], 1),\n    })\n\ndf = pd.DataFrame(percentage_table)\n\nprint(tabulate(df, headers='keys', tablefmt='pretty', showindex=False, colalign=(\"left\",) * len(df.columns)))","metadata":{"execution":{"iopub.status.busy":"2024-09-29T12:27:14.963769Z","iopub.execute_input":"2024-09-29T12:27:14.964662Z","iopub.status.idle":"2024-09-29T12:27:14.983970Z","shell.execute_reply.started":"2024-09-29T12:27:14.964620Z","shell.execute_reply":"2024-09-29T12:27:14.983004Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stdout","text":"+------------+---------------------------------------+------------------+--------------------+---------------------------------------+\n| Class      | Correct Original, Incorrect Canny (%) | Correct Both (%) | Incorrect Both (%) | Correct Canny, Incorrect Original (%) |\n+------------+---------------------------------------+------------------+--------------------+---------------------------------------+\n| cane       | 0.0                                   | 0.0              | 99.0               | 1.0                                   |\n| cavallo    | 58.0                                  | 26.0             | 12.0               | 4.0                                   |\n| elefante   | 23.0                                  | 77.0             | 0.0                | 0.0                                   |\n| farfalla   | 58.0                                  | 34.0             | 6.0                | 2.0                                   |\n| gallina    | 68.0                                  | 29.0             | 2.0                | 1.0                                   |\n| gatto      | 47.0                                  | 51.0             | 1.0                | 1.0                                   |\n| mucca      | 60.0                                  | 25.0             | 13.0               | 2.0                                   |\n| pecora     | 1.0                                   | 0.0              | 98.0               | 1.0                                   |\n| ragno      | 5.0                                   | 6.0              | 45.0               | 44.0                                  |\n| scoiattolo | 30.0                                  | 4.0              | 56.0               | 10.0                                  |\n+------------+---------------------------------------+------------------+--------------------+---------------------------------------+\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Colour and Texture Bias","metadata":{}},{"cell_type":"code","source":"import torch\nimport clip\nfrom PIL import Image\nimport os\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-09-29T13:12:15.255399Z","iopub.execute_input":"2024-09-29T13:12:15.256306Z","iopub.status.idle":"2024-09-29T13:12:15.260540Z","shell.execute_reply.started":"2024-09-29T13:12:15.256262Z","shell.execute_reply":"2024-09-29T13:12:15.259631Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel, preprocess = clip.load(\"ViT-B/32\", device=device)\n\noriginal_dir = '/kaggle/input/animal10-subset/original_subset/small_animal_dataset_updated'\ncolour_dir = '/kaggle/input/animal10-subset/decolourized_subset/'\ntexture_dir = '/kaggle/input/animal10-subset/textured_subset/Stylized_images'\n# in the textured dir the class names are like cane_stylized, cavallo_stylized etc. whereas in the other two they are like cane, cavallo etc.","metadata":{"execution":{"iopub.status.busy":"2024-09-29T13:13:09.158375Z","iopub.execute_input":"2024-09-29T13:13:09.159012Z","iopub.status.idle":"2024-09-29T13:13:13.000281Z","shell.execute_reply.started":"2024-09-29T13:13:09.158974Z","shell.execute_reply":"2024-09-29T13:13:12.999420Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"def load_images_and_classes(directory, suffix=''):\n    images = []\n    classes = []\n    for class_name in os.listdir(directory):\n        class_path = os.path.join(directory, class_name)\n        if os.path.isdir(class_path):\n            for img_file in os.listdir(class_path):\n                if img_file.endswith(('.png', '.jpg', '.jpeg')):\n                    img_path = os.path.join(class_path, img_file)\n                    images.append(img_path)\n                    classes.append(class_name + suffix)  # Append suffix if needed\n    return images, classes\n\noriginal_images, original_classes = load_images_and_classes(original_dir)\n\ncolour_images, colour_classes = load_images_and_classes(colour_dir)\n\ntexture_images, texture_classes = load_images_and_classes(texture_dir, suffix='_stylized')","metadata":{"execution":{"iopub.status.busy":"2024-09-29T13:13:15.116228Z","iopub.execute_input":"2024-09-29T13:13:15.116620Z","iopub.status.idle":"2024-09-29T13:13:15.238618Z","shell.execute_reply.started":"2024-09-29T13:13:15.116581Z","shell.execute_reply":"2024-09-29T13:13:15.237568Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"def calculate_accuracy(images, classes):\n    correct = 0\n    total = len(images)\n    \n    class_names = list(set(classes))\n    \n    text_inputs = clip.tokenize([f\"a photo of {class_name}\" for class_name in class_names]).to(device)\n\n    with torch.no_grad():\n        text_embeddings = model.encode_text(text_inputs)\n\n    for img_path, class_name in zip(images, classes):\n        \n        image = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n\n        with torch.no_grad():\n            image_embedding = model.encode_image(image)\n\n        similarities = (image_embedding @ text_embeddings.T).squeeze(0)\n        \n        predicted_class_index = similarities.argmax().item()\n        predicted_class = class_names[predicted_class_index]\n\n        if predicted_class == class_name:\n            correct += 1\n\n    accuracy = correct / total * 100\n    return accuracy","metadata":{"execution":{"iopub.status.busy":"2024-09-29T13:23:07.704123Z","iopub.execute_input":"2024-09-29T13:23:07.704558Z","iopub.status.idle":"2024-09-29T13:23:07.712784Z","shell.execute_reply.started":"2024-09-29T13:23:07.704518Z","shell.execute_reply":"2024-09-29T13:23:07.711871Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"original_accuracy = calculate_accuracy(original_images, original_classes)\nprint(f'Original Accuracy: {original_accuracy:.2f}%')\n\ncolour_accuracy = calculate_accuracy(colour_images, colour_classes)\nprint(f'Colour Accuracy: {colour_accuracy:.2f}%')\n\ntexture_accuracy = calculate_accuracy(texture_images, texture_classes)\nprint(f'Texture Accuracy: {texture_accuracy:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-09-29T13:23:10.278886Z","iopub.execute_input":"2024-09-29T13:23:10.279800Z","iopub.status.idle":"2024-09-29T13:23:37.348602Z","shell.execute_reply.started":"2024-09-29T13:23:10.279756Z","shell.execute_reply":"2024-09-29T13:23:37.347665Z"},"trusted":true},"execution_count":82,"outputs":[{"name":"stdout","text":"Original Accuracy: 65.01%\nColour Accuracy: 61.60%\nTexture Accuracy: 37.14%\n","output_type":"stream"}]},{"cell_type":"code","source":"colour_bias = colour_accuracy / original_accuracy\ntexture_bias = texture_accuracy / original_accuracy\nprint(f'Colour Bias: {colour_bias}')\nprint(f'Texture Bias: {texture_bias}')","metadata":{"execution":{"iopub.status.busy":"2024-09-29T13:23:44.059488Z","iopub.execute_input":"2024-09-29T13:23:44.060200Z","iopub.status.idle":"2024-09-29T13:23:44.065260Z","shell.execute_reply.started":"2024-09-29T13:23:44.060156Z","shell.execute_reply":"2024-09-29T13:23:44.064361Z"},"trusted":true},"execution_count":83,"outputs":[{"name":"stdout","text":"Colour Bias: 0.9475444617784712\nTexture Bias: 0.5713394250055717\n","output_type":"stream"}]}]}