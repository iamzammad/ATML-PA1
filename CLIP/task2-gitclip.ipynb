{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9503961,"sourceType":"datasetVersion","datasetId":5784356}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!conda install --yes -c pytorch pytorch=1.7.1 torchvision cudatoolkit=11.0\n!pip install ftfy regex tqdm\n!pip install git+https://github.com/openai/CLIP.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-29T09:52:39.201670Z","iopub.execute_input":"2024-09-29T09:52:39.202015Z","iopub.status.idle":"2024-09-29T10:17:27.170775Z","shell.execute_reply.started":"2024-09-29T09:52:39.201977Z","shell.execute_reply":"2024-09-29T10:17:27.169675Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Retrieving notices: ...working... done\nChannels:\n - pytorch\n - rapidsai\n - nvidia\n - nodefaults\n - conda-forge\n - defaults\nPlatform: linux-64\nCollecting package metadata (repodata.json): done\nSolving environment: | \u001b[33m\u001b[1mwarning  libmamba\u001b[m Added empty dependency for problem type SOLVER_RULE_UPDATE\nfailed\n\nLibMambaUnsatisfiableError: Encountered problems while solving:\n  - package pytorch-1.7.1-py3.6_cpu_0 requires python >=3.6,<3.7.0a0, but none of the providers can be installed\n  - package cuda-version-12.3-h32bc705_3 has constraint cudatoolkit 12.3|12.3.* conflicting with cudatoolkit-11.0.221-h6bb024c_0\n\nCould not solve for environment specs\nThe following packages are incompatible\n├─ \u001b[32mcuda-version 12.3** \u001b[0m is installable and it requires\n│  └─ \u001b[32mcudatoolkit 12.3|12.3.* \u001b[0m, which can be installed;\n├─ \u001b[31mcudatoolkit 11.0** \u001b[0m is not installable because it conflicts with any installable versions previously reported;\n├─ \u001b[32mpin-1\u001b[0m is installable and it requires\n│  └─ \u001b[32mpython 3.10.* \u001b[0m, which can be installed;\n└─ \u001b[31mpytorch 1.7.1** \u001b[0m is not installable because there are no viable options\n   ├─ \u001b[31mpytorch 1.7.1\u001b[0m would require\n   │  └─ \u001b[31mpython >=3.6,<3.7.0a0 \u001b[0m, which conflicts with any installable versions previously reported;\n   ├─ \u001b[31mpytorch 1.7.1\u001b[0m would require\n   │  └─ \u001b[31mpython >=3.7,<3.8.0a0 \u001b[0m, which conflicts with any installable versions previously reported;\n   ├─ \u001b[31mpytorch 1.7.1\u001b[0m would require\n   │  └─ \u001b[31mpython >=3.8,<3.9.0a0 \u001b[0m, which conflicts with any installable versions previously reported;\n   └─ \u001b[31mpytorch 1.7.1\u001b[0m would require\n      └─ \u001b[31mpython >=3.9,<3.10.0a0 \u001b[0m, which conflicts with any installable versions previously reported.\n\nCollecting ftfy\n  Downloading ftfy-6.2.3-py3-none-any.whl.metadata (7.8 kB)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (2024.5.15)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.4)\nRequirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/conda/lib/python3.10/site-packages (from ftfy) (0.2.13)\nDownloading ftfy-6.2.3-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.0/43.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: ftfy\nSuccessfully installed ftfy-6.2.3\nCollecting git+https://github.com/openai/CLIP.git\n  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-le__wil1\n  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-le__wil1\n  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: ftfy in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (6.2.3)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (21.3)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (2024.5.15)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (4.66.4)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (2.4.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (0.19.0)\nRequirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/conda/lib/python3.10/site-packages (from ftfy->clip==1.0) (0.2.13)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->clip==1.0) (3.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (2024.6.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (10.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->clip==1.0) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->clip==1.0) (1.3.0)\nBuilding wheels for collected packages: clip\n  Building wheel for clip (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=766c3339b40560d41c47d1019dd05afd86dbb69ac6b1232d84163c8e8bccf578\n  Stored in directory: /tmp/pip-ephem-wheel-cache-kktw61z3/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\nSuccessfully built clip\nInstalling collected packages: clip\nSuccessfully installed clip-1.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import clip\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-09-29T10:34:47.428081Z","iopub.execute_input":"2024-09-29T10:34:47.428550Z","iopub.status.idle":"2024-09-29T10:34:54.317081Z","shell.execute_reply.started":"2024-09-29T10:34:47.428510Z","shell.execute_reply":"2024-09-29T10:34:54.316256Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel, preprocess = clip.load(\"ViT-B/32\", device=device)\n\nimage = preprocess(Image.open(\"/kaggle/input/cat-image/cat1.jpg\")).unsqueeze(0).to(device)\ntext = clip.tokenize([\"a cute cat\", \"a normal cat\", \"a cat\"]).to(device)\n\nwith torch.no_grad():\n    image_features = model.encode_image(image)\n    text_features = model.encode_text(text)\n    \n    logits_per_image, logits_per_text = model(image, text)\n    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n\nprint(\"Label probs:\", probs) ","metadata":{"execution":{"iopub.status.busy":"2024-09-29T10:43:22.262870Z","iopub.execute_input":"2024-09-29T10:43:22.263253Z","iopub.status.idle":"2024-09-29T10:43:26.544477Z","shell.execute_reply.started":"2024-09-29T10:43:22.263219Z","shell.execute_reply":"2024-09-29T10:43:26.543345Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Label probs: [[0.7754  0.06464 0.16   ]]\n","output_type":"stream"}]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\npreprocess = transforms.Compose([\n    transforms.Resize((224, 224)),  \n    transforms.ToTensor(), \n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  \n])\n\ntestset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=preprocess)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)\n\ncifar10_classes = [\n    'airplane', 'automobile', 'bird', 'cat', 'deer',\n    'dog', 'frog', 'horse', 'ship', 'truck'\n]","metadata":{"execution":{"iopub.status.busy":"2024-09-29T10:56:26.792170Z","iopub.execute_input":"2024-09-29T10:56:26.793298Z","iopub.status.idle":"2024-09-29T10:56:27.709347Z","shell.execute_reply.started":"2024-09-29T10:56:26.793255Z","shell.execute_reply":"2024-09-29T10:56:27.708462Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Files already downloaded and verified\n","output_type":"stream"}]},{"cell_type":"code","source":"model, preprocess = clip.load(\"ViT-B/32\", device=device)\ntext_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in cifar10_classes]).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-09-29T10:56:29.834322Z","iopub.execute_input":"2024-09-29T10:56:29.834973Z","iopub.status.idle":"2024-09-29T10:56:34.108982Z","shell.execute_reply.started":"2024-09-29T10:56:29.834933Z","shell.execute_reply":"2024-09-29T10:56:34.107982Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, testloader, text_inputs):\n    total = 0\n    correct = 0\n\n    model.eval()\n\n    with torch.no_grad():\n        for images, labels in tqdm(testloader):\n            \n            images = images.to(device)\n            labels = labels.to(device)\n\n            image_features = model.encode_image(images)\n\n            image_features /= image_features.norm(dim=-1, keepdim=True)\n\n            text_features = model.encode_text(text_inputs)\n\n            text_features /= text_features.norm(dim=-1, keepdim=True)\n\n            similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n\n            _, predicted = similarity.max(dim=1)\n\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    return correct / total\n\naccuracy = evaluate(model, testloader, text_inputs)\nprint(f\"Accuracy of CLIP on CIFAR-10: {accuracy:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-29T10:56:38.667137Z","iopub.execute_input":"2024-09-29T10:56:38.667525Z","iopub.status.idle":"2024-09-29T10:58:18.570214Z","shell.execute_reply.started":"2024-09-29T10:56:38.667480Z","shell.execute_reply":"2024-09-29T10:58:18.569046Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"100%|██████████| 782/782 [01:39<00:00,  7.83it/s]","output_type":"stream"},{"name":"stdout","text":"Accuracy of CLIP on CIFAR-10: 0.8514\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]}]}