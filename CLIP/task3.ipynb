{"cells":[{"cell_type":"code","execution_count":5,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-09-26T19:36:45.881344Z","iopub.status.busy":"2024-09-26T19:36:45.880809Z","iopub.status.idle":"2024-09-26T19:36:45.887386Z","shell.execute_reply":"2024-09-26T19:36:45.886260Z","shell.execute_reply.started":"2024-09-26T19:36:45.881298Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","from torch.utils.data import DataLoader\n","from transformers import CLIPModel, CLIPProcessor\n","import os\n","from torchvision.datasets import ImageFolder"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-09-26T19:21:15.581460Z","iopub.status.busy":"2024-09-26T19:21:15.580906Z","iopub.status.idle":"2024-09-26T19:21:52.235837Z","shell.execute_reply":"2024-09-26T19:21:52.234731Z","shell.execute_reply.started":"2024-09-26T19:21:15.581419Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading http://ufldl.stanford.edu/housenumbers/train_32x32.mat to ./data/train_32x32.mat\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 182040794/182040794 [00:20<00:00, 8878409.02it/s] \n"]},{"name":"stdout","output_type":"stream","text":["Downloading http://ufldl.stanford.edu/housenumbers/test_32x32.mat to ./data/test_32x32.mat\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 64275384/64275384 [00:11<00:00, 5446918.11it/s]\n"]}],"source":["transform = transforms.Compose([\n","    transforms.ToTensor(),\n","])\n","\n","train_dataset = datasets.SVHN(root='./data', download=True, split='train', transform=transform)\n","test_dataset = datasets.SVHN(root='./data', download=True, split='test', transform=transform)\n","\n","train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-09-26T19:33:08.265646Z","iopub.status.busy":"2024-09-26T19:33:08.264544Z","iopub.status.idle":"2024-09-26T19:33:31.949636Z","shell.execute_reply":"2024-09-26T19:33:31.948510Z","shell.execute_reply.started":"2024-09-26T19:33:08.265602Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a44260c78be7458ca86e8eb2fb182448","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/4.10k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"da8de4c8ecc249d0971f84fe7ebe36bd","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/599M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0eadbfd76f484df0a6e171f6270cf382","version_major":2,"version_minor":0},"text/plain":["preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"35a400e82991494986dc63a2a6abbb4f","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/905 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"53261ffe5fbe4d1ea699dbaa7b9f8055","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/961k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"86ccc4ef78d04a5493abe6870402405c","version_major":2,"version_minor":0},"text/plain":["merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"66236ebc350b4c4791aeb26e8418ca6e","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"875b53df37584188a235fd19781f314a","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]},{"data":{"text/plain":["CLIPModel(\n","  (text_model): CLIPTextTransformer(\n","    (embeddings): CLIPTextEmbeddings(\n","      (token_embedding): Embedding(49408, 512)\n","      (position_embedding): Embedding(77, 512)\n","    )\n","    (encoder): CLIPEncoder(\n","      (layers): ModuleList(\n","        (0-11): 12 x CLIPEncoderLayer(\n","          (self_attn): CLIPSdpaAttention(\n","            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (mlp): CLIPMLP(\n","            (activation_fn): QuickGELUActivation()\n","            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","          )\n","          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","    )\n","    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (vision_model): CLIPVisionTransformer(\n","    (embeddings): CLIPVisionEmbeddings(\n","      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n","      (position_embedding): Embedding(197, 768)\n","    )\n","    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    (encoder): CLIPEncoder(\n","      (layers): ModuleList(\n","        (0-11): 12 x CLIPEncoderLayer(\n","          (self_attn): CLIPSdpaAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (mlp): CLIPMLP(\n","            (activation_fn): QuickGELUActivation()\n","            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","    )\n","    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n","  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",")"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n","processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n","model.eval()"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-09-26T19:34:53.755692Z","iopub.status.busy":"2024-09-26T19:34:53.754541Z","iopub.status.idle":"2024-09-26T19:34:53.767933Z","shell.execute_reply":"2024-09-26T19:34:53.766676Z","shell.execute_reply.started":"2024-09-26T19:34:53.755648Z"},"trusted":true},"outputs":[],"source":["def evaluate_clip_model(model, processor, data_loader):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)  \n","\n","    correct = 0\n","    total = 0\n","\n","    text_labels = [f\"This is a digit {i}\" for i in range(10)]\n","    \n","    text_inputs = processor(text=text_labels, return_tensors=\"pt\", padding=True)\n","    text_inputs = {k: v.to(device) for k, v in text_inputs.items()}  \n","\n","    model.eval()  \n","    with torch.no_grad():\n","        for images, labels in data_loader:\n","            images = images.to(device)\n","            labels = labels.to(device)\n","\n","            inputs = processor(images=images, return_tensors=\"pt\", padding=True, do_rescale=False)\n","            inputs = {k: v.to(device) for k, v in inputs.items()}  \n","\n","            outputs = model(pixel_values=inputs['pixel_values'],\n","                            input_ids=text_inputs['input_ids'],\n","                            attention_mask=text_inputs['attention_mask'])\n","\n","            logits_per_image = outputs.logits_per_image\n","            predicted = torch.argmax(logits_per_image, dim=1)\n","\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    accuracy = 100 * correct / total\n","    return accuracy\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-09-26T09:11:17.188126Z","iopub.status.busy":"2024-09-26T09:11:17.187216Z","iopub.status.idle":"2024-09-26T09:15:17.503467Z","shell.execute_reply":"2024-09-26T09:15:17.502465Z","shell.execute_reply.started":"2024-09-26T09:11:17.188085Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model Accuracy on SVHN using CLIP: 44.06%\n"]}],"source":["svhn_accuracy = evaluate_clip_model(model, processor, test_loader)\n","print(f'Model Accuracy on SVHN using CLIP: {svhn_accuracy:.2f}%')\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-09-26T09:32:22.999302Z","iopub.status.busy":"2024-09-26T09:32:22.998328Z","iopub.status.idle":"2024-09-26T09:35:48.282521Z","shell.execute_reply":"2024-09-26T09:35:48.281537Z","shell.execute_reply.started":"2024-09-26T09:32:22.999247Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading PACS dataset...\n","Entered get_pacs_loaders function\n","Number of photos: 1670\n","Number of art paintings: 2048\n","Number of cartoons: 2344\n","Number of sketches: 3929\n","Processing Photos...\n","Photos Accuracy: 11.32%\n","Processing Art_paintings...\n","Art_paintings Accuracy: 18.51%\n","Processing Cartoons...\n","Cartoons Accuracy: 16.60%\n","Processing Sketches...\n","Sketches Accuracy: 19.65%\n","\n","Overall category-wise accuracies:\n","Photos: 11.32%\n","Art_paintings: 18.51%\n","Cartoons: 16.60%\n","Sketches: 19.65%\n"]}],"source":["def get_pacs_loaders(batch_size=64):\n","    print(\"Entered get_pacs_loaders function\")\n","    \n","    transform = transforms.Compose([\n","        transforms.Resize((224, 224)),\n","        transforms.ToTensor(),\n","    ])\n","\n","    root_dir = '/kaggle/input/pacs-dataset/pacs_data/pacs_data'  \n","\n","    try:\n","        pacs_photos = ImageFolder(root=os.path.join(root_dir, 'photo'), transform=transform)\n","        pacs_art = ImageFolder(root=os.path.join(root_dir, 'art_painting'), transform=transform)\n","        pacs_cartoon = ImageFolder(root=os.path.join(root_dir, 'cartoon'), transform=transform)\n","        pacs_sketch = ImageFolder(root=os.path.join(root_dir, 'sketch'), transform=transform)\n","    except FileNotFoundError as e:\n","        print(f\"Error loading dataset: {e}\")\n","        return\n","\n","    loaders = {\n","        'Photos': DataLoader(pacs_photos, batch_size=batch_size, shuffle=True),\n","        'Art_paintings': DataLoader(pacs_art, batch_size=batch_size, shuffle=True),\n","        'Cartoons': DataLoader(pacs_cartoon, batch_size=batch_size, shuffle=True),\n","        'Sketches': DataLoader(pacs_sketch, batch_size=batch_size, shuffle=True),\n","    }\n","\n","    print(f\"Number of photos: {len(pacs_photos)}\")\n","    print(f\"Number of art paintings: {len(pacs_art)}\")\n","    print(f\"Number of cartoons: {len(pacs_cartoon)}\")\n","    print(f\"Number of sketches: {len(pacs_sketch)}\")\n","\n","    return loaders\n","\n","def run_clip_on_pacs(loaders):\n","    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n","    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n","\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","    model.to(device)\n","\n","    model.eval()\n","\n","    category_accuracies = {}\n","\n","    for category, loader in loaders.items():\n","        print(f\"Processing {category}...\")\n","        \n","        correct = 0\n","        total = 0\n","\n","        all_preds = []\n","        all_labels = []\n","\n","        for images, labels in loader:\n","            images = images.to(device)  \n","\n","            text_inputs = [category] * images.size(0)\n","            inputs = processor(text=text_inputs, images=images, return_tensors=\"pt\", padding=True)\n","\n","            for key in inputs.keys():\n","                inputs[key] = inputs[key].to(device)\n","\n","            with torch.no_grad():\n","                outputs = model(**inputs)\n","\n","            logits_per_image = outputs.logits_per_image  \n","            probs = logits_per_image.softmax(dim=1)  \n","\n","            preds = probs.argmax(dim=1)\n","            all_preds.extend(preds.cpu().numpy())  \n","            all_labels.extend(labels.cpu().numpy())  \n","\n","            total += labels.size(0)\n","            correct += (preds.cpu() == labels.cpu()).sum().item()\n","\n","        accuracy = 100 * correct / total\n","        category_accuracies[category] = accuracy\n","        print(f'{category} Accuracy: {accuracy:.2f}%')\n","\n","    # Print out the accuracy for each category\n","    print(\"\\nOverall category-wise accuracies:\")\n","    for category, acc in category_accuracies.items():\n","        print(f\"{category}: {acc:.2f}%\")\n","\n","if __name__ == \"__main__\":\n","    print(\"Loading PACS dataset...\")\n","    loaders = get_pacs_loaders()\n","    if loaders:\n","        run_clip_on_pacs(loaders)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":26161,"sourceId":33368,"sourceType":"datasetVersion"},{"datasetId":1733444,"sourceId":2834375,"sourceType":"datasetVersion"}],"dockerImageVersionId":30776,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
