{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":840806,"sourceType":"datasetVersion","datasetId":59760},{"sourceId":2834375,"sourceType":"datasetVersion","datasetId":1733444},{"sourceId":3032274,"sourceType":"datasetVersion","datasetId":1856924},{"sourceId":9501105,"sourceType":"datasetVersion","datasetId":5782230},{"sourceId":9501548,"sourceType":"datasetVersion","datasetId":5782563},{"sourceId":9501651,"sourceType":"datasetVersion","datasetId":5782635},{"sourceId":9501723,"sourceType":"datasetVersion","datasetId":5782691},{"sourceId":9504247,"sourceType":"datasetVersion","datasetId":5784558},{"sourceId":9504530,"sourceType":"datasetVersion","datasetId":5784751},{"sourceId":9504667,"sourceType":"datasetVersion","datasetId":5784854},{"sourceId":9507495,"sourceType":"datasetVersion","datasetId":5786820}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Task 0: Model Selection\n* Model Selected: VIT (Timms vit_base_patch16_224)\n* Pretrained on: ImageNet-21k\n* Parameters: 86.6M","metadata":{}},{"cell_type":"code","source":"!git clone https://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@github.com/iamzammad/ATML-PA1.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-29T19:59:44.025213Z","iopub.execute_input":"2024-09-29T19:59:44.026139Z","iopub.status.idle":"2024-09-29T19:59:45.025743Z","shell.execute_reply.started":"2024-09-29T19:59:44.026092Z","shell.execute_reply":"2024-09-29T19:59:45.024765Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"fatal: destination path 'ATML-PA1' already exists and is not an empty directory.\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd /kaggle/working/ATML-PA1/VIT\n!git pull origin main","metadata":{"execution":{"iopub.status.busy":"2024-09-29T19:59:48.457195Z","iopub.execute_input":"2024-09-29T19:59:48.457929Z","iopub.status.idle":"2024-09-29T19:59:50.448469Z","shell.execute_reply.started":"2024-09-29T19:59:48.457887Z","shell.execute_reply":"2024-09-29T19:59:50.447489Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"/kaggle/working/ATML-PA1/VIT\nFrom https://github.com/iamzammad/ATML-PA1\n * branch            main       -> FETCH_HEAD\nAlready up to date.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Task 2: Evaluation on an IID Dataset","metadata":{}},{"cell_type":"markdown","source":"**Accuracy of Pre-trained VIT on Cifar-10**","metadata":{}},{"cell_type":"code","source":"from inference import load_and_evaluate\nload_and_evaluate(model_type='pretrained')","metadata":{"execution":{"iopub.status.busy":"2024-09-29T20:00:57.712816Z","iopub.execute_input":"2024-09-29T20:00:57.713243Z","iopub.status.idle":"2024-09-29T20:02:03.843727Z","shell.execute_reply.started":"2024-09-29T20:00:57.713202Z","shell.execute_reply":"2024-09-29T20:02:03.842493Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\nEvaluating pretrained model...\nAccuracy on CIFAR-10 test set: 7.50%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Fine-Tuning VIT on Cifar-10**","metadata":{}},{"cell_type":"code","source":"!python Fine_Tune.py","metadata":{"execution":{"iopub.status.busy":"2024-09-29T20:03:19.519978Z","iopub.execute_input":"2024-09-29T20:03:19.520767Z","iopub.status.idle":"2024-09-29T20:17:50.801563Z","shell.execute_reply.started":"2024-09-29T20:03:19.520720Z","shell.execute_reply":"2024-09-29T20:17:50.800424Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\nvit.cls_token False\nvit.pos_embed False\nvit.patch_embed.proj.weight False\nvit.patch_embed.proj.bias False\nvit.blocks.0.norm1.weight False\nvit.blocks.0.norm1.bias False\nvit.blocks.0.attn.qkv.weight False\nvit.blocks.0.attn.qkv.bias False\nvit.blocks.0.attn.proj.weight False\nvit.blocks.0.attn.proj.bias False\nvit.blocks.0.norm2.weight False\nvit.blocks.0.norm2.bias False\nvit.blocks.0.mlp.fc1.weight False\nvit.blocks.0.mlp.fc1.bias False\nvit.blocks.0.mlp.fc2.weight False\nvit.blocks.0.mlp.fc2.bias False\nvit.blocks.1.norm1.weight False\nvit.blocks.1.norm1.bias False\nvit.blocks.1.attn.qkv.weight False\nvit.blocks.1.attn.qkv.bias False\nvit.blocks.1.attn.proj.weight False\nvit.blocks.1.attn.proj.bias False\nvit.blocks.1.norm2.weight False\nvit.blocks.1.norm2.bias False\nvit.blocks.1.mlp.fc1.weight False\nvit.blocks.1.mlp.fc1.bias False\nvit.blocks.1.mlp.fc2.weight False\nvit.blocks.1.mlp.fc2.bias False\nvit.blocks.2.norm1.weight False\nvit.blocks.2.norm1.bias False\nvit.blocks.2.attn.qkv.weight False\nvit.blocks.2.attn.qkv.bias False\nvit.blocks.2.attn.proj.weight False\nvit.blocks.2.attn.proj.bias False\nvit.blocks.2.norm2.weight False\nvit.blocks.2.norm2.bias False\nvit.blocks.2.mlp.fc1.weight False\nvit.blocks.2.mlp.fc1.bias False\nvit.blocks.2.mlp.fc2.weight False\nvit.blocks.2.mlp.fc2.bias False\nvit.blocks.3.norm1.weight False\nvit.blocks.3.norm1.bias False\nvit.blocks.3.attn.qkv.weight False\nvit.blocks.3.attn.qkv.bias False\nvit.blocks.3.attn.proj.weight False\nvit.blocks.3.attn.proj.bias False\nvit.blocks.3.norm2.weight False\nvit.blocks.3.norm2.bias False\nvit.blocks.3.mlp.fc1.weight False\nvit.blocks.3.mlp.fc1.bias False\nvit.blocks.3.mlp.fc2.weight False\nvit.blocks.3.mlp.fc2.bias False\nvit.blocks.4.norm1.weight False\nvit.blocks.4.norm1.bias False\nvit.blocks.4.attn.qkv.weight False\nvit.blocks.4.attn.qkv.bias False\nvit.blocks.4.attn.proj.weight False\nvit.blocks.4.attn.proj.bias False\nvit.blocks.4.norm2.weight False\nvit.blocks.4.norm2.bias False\nvit.blocks.4.mlp.fc1.weight False\nvit.blocks.4.mlp.fc1.bias False\nvit.blocks.4.mlp.fc2.weight False\nvit.blocks.4.mlp.fc2.bias False\nvit.blocks.5.norm1.weight False\nvit.blocks.5.norm1.bias False\nvit.blocks.5.attn.qkv.weight False\nvit.blocks.5.attn.qkv.bias False\nvit.blocks.5.attn.proj.weight False\nvit.blocks.5.attn.proj.bias False\nvit.blocks.5.norm2.weight False\nvit.blocks.5.norm2.bias False\nvit.blocks.5.mlp.fc1.weight False\nvit.blocks.5.mlp.fc1.bias False\nvit.blocks.5.mlp.fc2.weight False\nvit.blocks.5.mlp.fc2.bias False\nvit.blocks.6.norm1.weight False\nvit.blocks.6.norm1.bias False\nvit.blocks.6.attn.qkv.weight False\nvit.blocks.6.attn.qkv.bias False\nvit.blocks.6.attn.proj.weight False\nvit.blocks.6.attn.proj.bias False\nvit.blocks.6.norm2.weight False\nvit.blocks.6.norm2.bias False\nvit.blocks.6.mlp.fc1.weight False\nvit.blocks.6.mlp.fc1.bias False\nvit.blocks.6.mlp.fc2.weight False\nvit.blocks.6.mlp.fc2.bias False\nvit.blocks.7.norm1.weight False\nvit.blocks.7.norm1.bias False\nvit.blocks.7.attn.qkv.weight False\nvit.blocks.7.attn.qkv.bias False\nvit.blocks.7.attn.proj.weight False\nvit.blocks.7.attn.proj.bias False\nvit.blocks.7.norm2.weight False\nvit.blocks.7.norm2.bias False\nvit.blocks.7.mlp.fc1.weight False\nvit.blocks.7.mlp.fc1.bias False\nvit.blocks.7.mlp.fc2.weight False\nvit.blocks.7.mlp.fc2.bias False\nvit.blocks.8.norm1.weight False\nvit.blocks.8.norm1.bias False\nvit.blocks.8.attn.qkv.weight False\nvit.blocks.8.attn.qkv.bias False\nvit.blocks.8.attn.proj.weight False\nvit.blocks.8.attn.proj.bias False\nvit.blocks.8.norm2.weight False\nvit.blocks.8.norm2.bias False\nvit.blocks.8.mlp.fc1.weight False\nvit.blocks.8.mlp.fc1.bias False\nvit.blocks.8.mlp.fc2.weight False\nvit.blocks.8.mlp.fc2.bias False\nvit.blocks.9.norm1.weight False\nvit.blocks.9.norm1.bias False\nvit.blocks.9.attn.qkv.weight False\nvit.blocks.9.attn.qkv.bias False\nvit.blocks.9.attn.proj.weight False\nvit.blocks.9.attn.proj.bias False\nvit.blocks.9.norm2.weight False\nvit.blocks.9.norm2.bias False\nvit.blocks.9.mlp.fc1.weight False\nvit.blocks.9.mlp.fc1.bias False\nvit.blocks.9.mlp.fc2.weight False\nvit.blocks.9.mlp.fc2.bias False\nvit.blocks.10.norm1.weight False\nvit.blocks.10.norm1.bias False\nvit.blocks.10.attn.qkv.weight False\nvit.blocks.10.attn.qkv.bias False\nvit.blocks.10.attn.proj.weight False\nvit.blocks.10.attn.proj.bias False\nvit.blocks.10.norm2.weight False\nvit.blocks.10.norm2.bias False\nvit.blocks.10.mlp.fc1.weight False\nvit.blocks.10.mlp.fc1.bias False\nvit.blocks.10.mlp.fc2.weight False\nvit.blocks.10.mlp.fc2.bias False\nvit.blocks.11.norm1.weight False\nvit.blocks.11.norm1.bias False\nvit.blocks.11.attn.qkv.weight False\nvit.blocks.11.attn.qkv.bias False\nvit.blocks.11.attn.proj.weight False\nvit.blocks.11.attn.proj.bias False\nvit.blocks.11.norm2.weight False\nvit.blocks.11.norm2.bias False\nvit.blocks.11.mlp.fc1.weight False\nvit.blocks.11.mlp.fc1.bias False\nvit.blocks.11.mlp.fc2.weight False\nvit.blocks.11.mlp.fc2.bias False\nvit.norm.weight False\nvit.norm.bias False\nvit.head.weight True\nvit.head.bias True\n/kaggle/working/ATML-PA1/VIT/Fine_Tune.py:15: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()\n/kaggle/working/ATML-PA1/VIT/Fine_Tune.py:27: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\nEpoch [1/3], Batch [20/782], Loss: 2.1326\nEpoch [1/3], Batch [40/782], Loss: 1.6261\nEpoch [1/3], Batch [60/782], Loss: 1.3702\nEpoch [1/3], Batch [80/782], Loss: 1.3092\nEpoch [1/3], Batch [100/782], Loss: 0.7792\nEpoch [1/3], Batch [120/782], Loss: 0.7456\nEpoch [1/3], Batch [140/782], Loss: 0.6424\nEpoch [1/3], Batch [160/782], Loss: 0.8048\nEpoch [1/3], Batch [180/782], Loss: 0.6280\nEpoch [1/3], Batch [200/782], Loss: 0.5385\nEpoch [1/3], Batch [220/782], Loss: 0.5318\nEpoch [1/3], Batch [240/782], Loss: 0.3034\nEpoch [1/3], Batch [260/782], Loss: 0.5376\nEpoch [1/3], Batch [280/782], Loss: 0.5422\nEpoch [1/3], Batch [300/782], Loss: 0.4804\nEpoch [1/3], Batch [320/782], Loss: 0.3645\nEpoch [1/3], Batch [340/782], Loss: 0.3777\nEpoch [1/3], Batch [360/782], Loss: 0.3229\nEpoch [1/3], Batch [380/782], Loss: 0.5069\nEpoch [1/3], Batch [400/782], Loss: 0.2982\nEpoch [1/3], Batch [420/782], Loss: 0.2156\nEpoch [1/3], Batch [440/782], Loss: 0.3398\nEpoch [1/3], Batch [460/782], Loss: 0.1978\nEpoch [1/3], Batch [480/782], Loss: 0.4448\nEpoch [1/3], Batch [500/782], Loss: 0.2415\nEpoch [1/3], Batch [520/782], Loss: 0.2290\nEpoch [1/3], Batch [540/782], Loss: 0.1739\nEpoch [1/3], Batch [560/782], Loss: 0.2582\nEpoch [1/3], Batch [580/782], Loss: 0.2136\nEpoch [1/3], Batch [600/782], Loss: 0.3100\nEpoch [1/3], Batch [620/782], Loss: 0.1839\nEpoch [1/3], Batch [640/782], Loss: 0.2290\nEpoch [1/3], Batch [660/782], Loss: 0.1692\nEpoch [1/3], Batch [680/782], Loss: 0.2487\nEpoch [1/3], Batch [700/782], Loss: 0.3045\nEpoch [1/3], Batch [720/782], Loss: 0.2509\nEpoch [1/3], Batch [740/782], Loss: 0.2337\nEpoch [1/3], Batch [760/782], Loss: 0.2375\nEpoch [1/3], Batch [780/782], Loss: 0.2397\nEpoch [1/3] completed in 287.81 seconds. Loss: 0.5163, Accuracy: 86.39%\nEpoch [2/3], Batch [20/782], Loss: 0.2889\nEpoch [2/3], Batch [40/782], Loss: 0.2428\nEpoch [2/3], Batch [60/782], Loss: 0.2149\nEpoch [2/3], Batch [80/782], Loss: 0.2596\nEpoch [2/3], Batch [100/782], Loss: 0.1382\nEpoch [2/3], Batch [120/782], Loss: 0.1802\nEpoch [2/3], Batch [140/782], Loss: 0.1745\nEpoch [2/3], Batch [160/782], Loss: 0.2672\nEpoch [2/3], Batch [180/782], Loss: 0.2627\nEpoch [2/3], Batch [200/782], Loss: 0.2043\nEpoch [2/3], Batch [220/782], Loss: 0.2494\nEpoch [2/3], Batch [240/782], Loss: 0.1778\nEpoch [2/3], Batch [260/782], Loss: 0.1808\nEpoch [2/3], Batch [280/782], Loss: 0.1913\nEpoch [2/3], Batch [300/782], Loss: 0.2131\nEpoch [2/3], Batch [320/782], Loss: 0.1934\nEpoch [2/3], Batch [340/782], Loss: 0.2868\nEpoch [2/3], Batch [360/782], Loss: 0.3058\nEpoch [2/3], Batch [380/782], Loss: 0.2232\nEpoch [2/3], Batch [400/782], Loss: 0.2692\nEpoch [2/3], Batch [420/782], Loss: 0.2118\nEpoch [2/3], Batch [440/782], Loss: 0.1805\nEpoch [2/3], Batch [460/782], Loss: 0.2267\nEpoch [2/3], Batch [480/782], Loss: 0.4014\nEpoch [2/3], Batch [500/782], Loss: 0.1829\nEpoch [2/3], Batch [520/782], Loss: 0.1039\nEpoch [2/3], Batch [540/782], Loss: 0.1540\nEpoch [2/3], Batch [560/782], Loss: 0.1403\nEpoch [2/3], Batch [580/782], Loss: 0.1929\nEpoch [2/3], Batch [600/782], Loss: 0.1312\nEpoch [2/3], Batch [620/782], Loss: 0.1790\nEpoch [2/3], Batch [640/782], Loss: 0.1400\nEpoch [2/3], Batch [660/782], Loss: 0.2888\nEpoch [2/3], Batch [680/782], Loss: 0.1487\nEpoch [2/3], Batch [700/782], Loss: 0.1349\nEpoch [2/3], Batch [720/782], Loss: 0.1696\nEpoch [2/3], Batch [740/782], Loss: 0.2283\nEpoch [2/3], Batch [760/782], Loss: 0.1605\nEpoch [2/3], Batch [780/782], Loss: 0.2641\nEpoch [2/3] completed in 286.59 seconds. Loss: 0.2029, Accuracy: 93.82%\nEpoch [3/3], Batch [20/782], Loss: 0.1857\nEpoch [3/3], Batch [40/782], Loss: 0.2288\nEpoch [3/3], Batch [60/782], Loss: 0.0907\nEpoch [3/3], Batch [80/782], Loss: 0.0973\nEpoch [3/3], Batch [100/782], Loss: 0.1967\nEpoch [3/3], Batch [120/782], Loss: 0.2341\nEpoch [3/3], Batch [140/782], Loss: 0.1512\nEpoch [3/3], Batch [160/782], Loss: 0.1038\nEpoch [3/3], Batch [180/782], Loss: 0.2102\nEpoch [3/3], Batch [200/782], Loss: 0.1587\nEpoch [3/3], Batch [220/782], Loss: 0.1788\nEpoch [3/3], Batch [240/782], Loss: 0.2139\nEpoch [3/3], Batch [260/782], Loss: 0.1221\nEpoch [3/3], Batch [280/782], Loss: 0.1525\nEpoch [3/3], Batch [300/782], Loss: 0.1106\nEpoch [3/3], Batch [320/782], Loss: 0.1684\nEpoch [3/3], Batch [340/782], Loss: 0.1905\nEpoch [3/3], Batch [360/782], Loss: 0.2998\nEpoch [3/3], Batch [380/782], Loss: 0.2064\nEpoch [3/3], Batch [400/782], Loss: 0.0773\nEpoch [3/3], Batch [420/782], Loss: 0.1131\nEpoch [3/3], Batch [440/782], Loss: 0.1127\nEpoch [3/3], Batch [460/782], Loss: 0.1440\nEpoch [3/3], Batch [480/782], Loss: 0.2577\nEpoch [3/3], Batch [500/782], Loss: 0.1844\nEpoch [3/3], Batch [520/782], Loss: 0.2305\nEpoch [3/3], Batch [540/782], Loss: 0.1030\nEpoch [3/3], Batch [560/782], Loss: 0.2319\nEpoch [3/3], Batch [580/782], Loss: 0.2322\nEpoch [3/3], Batch [600/782], Loss: 0.3115\nEpoch [3/3], Batch [620/782], Loss: 0.0540\nEpoch [3/3], Batch [640/782], Loss: 0.1744\nEpoch [3/3], Batch [660/782], Loss: 0.1391\nEpoch [3/3], Batch [680/782], Loss: 0.1622\nEpoch [3/3], Batch [700/782], Loss: 0.0741\nEpoch [3/3], Batch [720/782], Loss: 0.0813\nEpoch [3/3], Batch [740/782], Loss: 0.1313\nEpoch [3/3], Batch [760/782], Loss: 0.0523\nEpoch [3/3], Batch [780/782], Loss: 0.1189\nEpoch [3/3] completed in 286.91 seconds. Loss: 0.1713, Accuracy: 94.46%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Finetuned Accuracy Backbone frozen**","metadata":{}},{"cell_type":"code","source":"from inference import load_and_evaluate\nload_and_evaluate(model_type='fine-tuned')","metadata":{"execution":{"iopub.status.busy":"2024-09-29T20:21:59.354818Z","iopub.execute_input":"2024-09-29T20:21:59.355277Z","iopub.status.idle":"2024-09-29T20:23:01.407465Z","shell.execute_reply.started":"2024-09-29T20:21:59.355232Z","shell.execute_reply":"2024-09-29T20:23:01.406234Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\nEvaluating fine-tuned model...\nAccuracy on CIFAR-10 test set: 94.45%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Finetuned Accuracy Backbone Unfrozen**","metadata":{}},{"cell_type":"code","source":"!python inference.py","metadata":{"execution":{"iopub.status.busy":"2024-09-22T22:00:17.141612Z","iopub.execute_input":"2024-09-22T22:00:17.141985Z","iopub.status.idle":"2024-09-22T22:02:20.113625Z","shell.execute_reply.started":"2024-09-22T22:00:17.141948Z","shell.execute_reply":"2024-09-22T22:02:20.112458Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\nAccuracy on CIFAR-10 test set: 96.27%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Task 3: Evaluation for Domain Generalization","metadata":{}},{"cell_type":"code","source":"import os\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import ImageFolder\n\ndef get_pacs_loaders(batch_size=32):\n    print(\"Entered get_pacs_loaders function\")\n    \n    transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ])\n\n\n    root_dir = '/kaggle/input/pacs-dataset/pacs_data/pacs_data'  \n\n    try:\n        pacs_photos = ImageFolder(root=os.path.join(root_dir, 'photo'), transform=transform)\n        pacs_art = ImageFolder(root=os.path.join(root_dir, 'art_painting'), transform=transform)\n        pacs_cartoon = ImageFolder(root=os.path.join(root_dir, 'cartoon'), transform=transform)\n        pacs_sketch = ImageFolder(root=os.path.join(root_dir, 'sketch'), transform=transform)\n    except FileNotFoundError as e:\n        print(f\"Error loading dataset: {e}\")\n        return\n\n    loaders = {\n        'Photos': DataLoader(pacs_photos, batch_size=batch_size, shuffle=True),\n        'Art_paintings': DataLoader(pacs_art, batch_size=batch_size, shuffle=True),\n        'Cartoons': DataLoader(pacs_cartoon, batch_size=batch_size, shuffle=True),\n        'Sketches': DataLoader(pacs_sketch, batch_size=batch_size, shuffle=True),\n    }\n\n    print(f\"Number of photos: {len(pacs_photos)}\")\n    print(f\"Number of art paintings: {len(pacs_art)}\")\n    print(f\"Number of cartoons: {len(pacs_cartoon)}\")\n    print(f\"Number of sketches: {len(pacs_sketch)}\")\n\n    for category, loader in loaders.items():\n        images, labels = next(iter(loader))\n        print(f\"Sample from {category}:\")\n        print(f\"Image batch shape: {images.shape}\")\n        print(f\"Label batch shape: {labels.shape}\")\n    \n    return loaders\n\nif __name__ == \"__main__\":\n    print(\"Loading PACS dataset...\")\n    loaders = get_pacs_loaders()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-29T20:38:43.382092Z","iopub.execute_input":"2024-09-29T20:38:43.382624Z","iopub.status.idle":"2024-09-29T20:38:44.666267Z","shell.execute_reply.started":"2024-09-29T20:38:43.382585Z","shell.execute_reply":"2024-09-29T20:38:44.665234Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Loading PACS dataset...\nEntered get_pacs_loaders function\nNumber of photos: 1670\nNumber of art paintings: 2048\nNumber of cartoons: 2344\nNumber of sketches: 3929\nSample from Photos:\nImage batch shape: torch.Size([32, 3, 224, 224])\nLabel batch shape: torch.Size([32])\nSample from Art_paintings:\nImage batch shape: torch.Size([32, 3, 224, 224])\nLabel batch shape: torch.Size([32])\nSample from Cartoons:\nImage batch shape: torch.Size([32, 3, 224, 224])\nLabel batch shape: torch.Size([32])\nSample from Sketches:\nImage batch shape: torch.Size([32, 3, 224, 224])\nLabel batch shape: torch.Size([32])\n","output_type":"stream"}]},{"cell_type":"code","source":"import timm\nimport torch\nimport torch.nn as nn\n\n\nclass ViTClassifier(nn.Module):\n    def __init__(self, num_classes, pretrained=True, finetune=\"classifier\"):\n        super(ViTClassifier, self).__init__()\n        self.vit = timm.create_model('vit_base_patch16_224', pretrained=pretrained)\n        self.vit.head = nn.Linear(self.vit.head.in_features, num_classes)\n\n        if finetune == \"classifier\":\n            #freezing the backbone\n            for param in self.vit.parameters():\n                param.requires_grad = False\n            #unfreezing the classifier\n            for param in self.vit.head.parameters():\n                param.requires_grad = True\n\n    def forward(self, x):\n        return self.vit(x)\n\ndef load_vit_model(num_classes, device):\n    model = ViTClassifier(num_classes)\n    model = model.to(device)\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-09-29T20:41:00.556329Z","iopub.execute_input":"2024-09-29T20:41:00.557033Z","iopub.status.idle":"2024-09-29T20:41:00.564856Z","shell.execute_reply.started":"2024-09-29T20:41:00.556992Z","shell.execute_reply":"2024-09-29T20:41:00.563687Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import torch\n\ndef evaluate_model(model, data_loaders, device):\n    model.eval()\n    total_accuracy = {}\n    \n    with torch.no_grad():\n        for domain, loader in data_loaders.items():\n            correct = 0\n            total = 0\n            \n            for images, labels in loader:\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                _, predicted = torch.max(outputs.data, 1)\n                \n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n            \n            accuracy = 100 * correct / total\n            total_accuracy[domain] = accuracy\n            print(f\"Accuracy on {domain}: {accuracy:.2f}%\")\n    \n    return total_accuracy\n\ndef main():\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    pacs_loaders = get_pacs_loaders(batch_size=64)  \n    num_classes = 7 \n    model = load_vit_model(num_classes, device)\n    accuracy = evaluate_model(model, pacs_loaders, device)\n    print(\"Final Accuracies across domains:\", accuracy)\n\n# Run inference\nmain()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-29T20:49:50.189948Z","iopub.execute_input":"2024-09-29T20:49:50.190930Z","iopub.status.idle":"2024-09-29T20:51:18.865847Z","shell.execute_reply.started":"2024-09-29T20:49:50.190877Z","shell.execute_reply":"2024-09-29T20:51:18.864734Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Entered get_pacs_loaders function\nNumber of photos: 1670\nNumber of art paintings: 2048\nNumber of cartoons: 2344\nNumber of sketches: 3929\nSample from Photos:\nImage batch shape: torch.Size([64, 3, 224, 224])\nLabel batch shape: torch.Size([64])\nSample from Art_paintings:\nImage batch shape: torch.Size([64, 3, 224, 224])\nLabel batch shape: torch.Size([64])\nSample from Cartoons:\nImage batch shape: torch.Size([64, 3, 224, 224])\nLabel batch shape: torch.Size([64])\nSample from Sketches:\nImage batch shape: torch.Size([64, 3, 224, 224])\nLabel batch shape: torch.Size([64])\nAccuracy on Photos: 2.93%\nAccuracy on Art_paintings: 11.04%\nAccuracy on Cartoons: 3.75%\nAccuracy on Sketches: 9.88%\nFinal Accuracies across domains: {'Photos': 2.934131736526946, 'Art_paintings': 11.03515625, 'Cartoons': 3.7542662116040955, 'Sketches': 9.875286332400101}\n","output_type":"stream"}]},{"cell_type":"code","source":"!python inference.py","metadata":{"execution":{"iopub.status.busy":"2024-09-24T20:55:34.049460Z","iopub.execute_input":"2024-09-24T20:55:34.050259Z","iopub.status.idle":"2024-09-24T20:58:40.321026Z","shell.execute_reply.started":"2024-09-24T20:55:34.050213Z","shell.execute_reply":"2024-09-24T20:58:40.319925Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"Entered get_pacs_loaders function\nNumber of photos: 1670\nNumber of art paintings: 2048\nNumber of cartoons: 2344\nNumber of sketches: 3929\nSample from Photos:\nImage batch shape: torch.Size([64, 3, 224, 224])\nLabel batch shape: torch.Size([64])\nSample from Art_paintings:\nImage batch shape: torch.Size([64, 3, 224, 224])\nLabel batch shape: torch.Size([64])\nSample from Cartoons:\nImage batch shape: torch.Size([64, 3, 224, 224])\nLabel batch shape: torch.Size([64])\nSample from Sketches:\nImage batch shape: torch.Size([64, 3, 224, 224])\nLabel batch shape: torch.Size([64])\nmodel.safetensors: 100%|██████████████████████| 346M/346M [00:01<00:00, 181MB/s]\nAccuracy on Photos: 15.45%\nAccuracy on Art_paintings: 17.82%\nAccuracy on Cartoons: 17.19%\nAccuracy on Sketches: 25.83%\nFinal Accuracies: {'Photos': 15.449101796407186, 'Art_paintings': 17.822265625, 'Cartoons': 17.19283276450512, 'Sketches': 25.833545431407483}\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.cuda.amp import autocast, GradScaler\nimport time\n\n# Assume `load_vit_model` and `get_pacs_loaders` are already defined in cells\n\ndef train_model(model, train_loader, criterion, optimizer, device, num_epochs=3):\n    model.train()\n    scaler = GradScaler()  \n    \n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        start_time = time.time() \n        \n        for batch_idx, (inputs, labels) in enumerate(train_loader):\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            with autocast(): \n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n\n            running_loss += loss.item()\n            \n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n            if (batch_idx + 1) % 10 == 0:\n                print(f\"Epoch [{epoch + 1}/{num_epochs}], Batch [{batch_idx + 1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n\n        epoch_loss = running_loss / len(train_loader)\n        epoch_accuracy = 100 * correct / total\n        print(f\"Epoch [{epoch + 1}/{num_epochs}] completed in {time.time() - start_time:.2f} seconds. \"\n              f\"Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n\ndef main():\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    pacs_loaders = get_pacs_loaders(batch_size=64)\n    \n    # domain 'Photos'\n    train_loader = pacs_loaders['Photos']\n    num_classes = 7 \n    model = load_vit_model(num_classes, device)\n\n    for name, param in model.named_parameters():\n        print(name, param.requires_grad)\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n    train_model(model, train_loader, criterion, optimizer, device, num_epochs=3)\n\n    torch.save(model.state_dict(), 'fine_tuned_vit_pacs.pth')\n    print(\"Model fine-tuned and saved successfully.\")\n\nmain()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-29T20:53:07.577568Z","iopub.execute_input":"2024-09-29T20:53:07.578353Z","iopub.status.idle":"2024-09-29T20:53:53.270101Z","shell.execute_reply.started":"2024-09-29T20:53:07.578314Z","shell.execute_reply":"2024-09-29T20:53:53.269090Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Entered get_pacs_loaders function\nNumber of photos: 1670\nNumber of art paintings: 2048\nNumber of cartoons: 2344\nNumber of sketches: 3929\nSample from Photos:\nImage batch shape: torch.Size([64, 3, 224, 224])\nLabel batch shape: torch.Size([64])\nSample from Art_paintings:\nImage batch shape: torch.Size([64, 3, 224, 224])\nLabel batch shape: torch.Size([64])\nSample from Cartoons:\nImage batch shape: torch.Size([64, 3, 224, 224])\nLabel batch shape: torch.Size([64])\nSample from Sketches:\nImage batch shape: torch.Size([64, 3, 224, 224])\nLabel batch shape: torch.Size([64])\nvit.cls_token False\nvit.pos_embed False\nvit.patch_embed.proj.weight False\nvit.patch_embed.proj.bias False\nvit.blocks.0.norm1.weight False\nvit.blocks.0.norm1.bias False\nvit.blocks.0.attn.qkv.weight False\nvit.blocks.0.attn.qkv.bias False\nvit.blocks.0.attn.proj.weight False\nvit.blocks.0.attn.proj.bias False\nvit.blocks.0.norm2.weight False\nvit.blocks.0.norm2.bias False\nvit.blocks.0.mlp.fc1.weight False\nvit.blocks.0.mlp.fc1.bias False\nvit.blocks.0.mlp.fc2.weight False\nvit.blocks.0.mlp.fc2.bias False\nvit.blocks.1.norm1.weight False\nvit.blocks.1.norm1.bias False\nvit.blocks.1.attn.qkv.weight False\nvit.blocks.1.attn.qkv.bias False\nvit.blocks.1.attn.proj.weight False\nvit.blocks.1.attn.proj.bias False\nvit.blocks.1.norm2.weight False\nvit.blocks.1.norm2.bias False\nvit.blocks.1.mlp.fc1.weight False\nvit.blocks.1.mlp.fc1.bias False\nvit.blocks.1.mlp.fc2.weight False\nvit.blocks.1.mlp.fc2.bias False\nvit.blocks.2.norm1.weight False\nvit.blocks.2.norm1.bias False\nvit.blocks.2.attn.qkv.weight False\nvit.blocks.2.attn.qkv.bias False\nvit.blocks.2.attn.proj.weight False\nvit.blocks.2.attn.proj.bias False\nvit.blocks.2.norm2.weight False\nvit.blocks.2.norm2.bias False\nvit.blocks.2.mlp.fc1.weight False\nvit.blocks.2.mlp.fc1.bias False\nvit.blocks.2.mlp.fc2.weight False\nvit.blocks.2.mlp.fc2.bias False\nvit.blocks.3.norm1.weight False\nvit.blocks.3.norm1.bias False\nvit.blocks.3.attn.qkv.weight False\nvit.blocks.3.attn.qkv.bias False\nvit.blocks.3.attn.proj.weight False\nvit.blocks.3.attn.proj.bias False\nvit.blocks.3.norm2.weight False\nvit.blocks.3.norm2.bias False\nvit.blocks.3.mlp.fc1.weight False\nvit.blocks.3.mlp.fc1.bias False\nvit.blocks.3.mlp.fc2.weight False\nvit.blocks.3.mlp.fc2.bias False\nvit.blocks.4.norm1.weight False\nvit.blocks.4.norm1.bias False\nvit.blocks.4.attn.qkv.weight False\nvit.blocks.4.attn.qkv.bias False\nvit.blocks.4.attn.proj.weight False\nvit.blocks.4.attn.proj.bias False\nvit.blocks.4.norm2.weight False\nvit.blocks.4.norm2.bias False\nvit.blocks.4.mlp.fc1.weight False\nvit.blocks.4.mlp.fc1.bias False\nvit.blocks.4.mlp.fc2.weight False\nvit.blocks.4.mlp.fc2.bias False\nvit.blocks.5.norm1.weight False\nvit.blocks.5.norm1.bias False\nvit.blocks.5.attn.qkv.weight False\nvit.blocks.5.attn.qkv.bias False\nvit.blocks.5.attn.proj.weight False\nvit.blocks.5.attn.proj.bias False\nvit.blocks.5.norm2.weight False\nvit.blocks.5.norm2.bias False\nvit.blocks.5.mlp.fc1.weight False\nvit.blocks.5.mlp.fc1.bias False\nvit.blocks.5.mlp.fc2.weight False\nvit.blocks.5.mlp.fc2.bias False\nvit.blocks.6.norm1.weight False\nvit.blocks.6.norm1.bias False\nvit.blocks.6.attn.qkv.weight False\nvit.blocks.6.attn.qkv.bias False\nvit.blocks.6.attn.proj.weight False\nvit.blocks.6.attn.proj.bias False\nvit.blocks.6.norm2.weight False\nvit.blocks.6.norm2.bias False\nvit.blocks.6.mlp.fc1.weight False\nvit.blocks.6.mlp.fc1.bias False\nvit.blocks.6.mlp.fc2.weight False\nvit.blocks.6.mlp.fc2.bias False\nvit.blocks.7.norm1.weight False\nvit.blocks.7.norm1.bias False\nvit.blocks.7.attn.qkv.weight False\nvit.blocks.7.attn.qkv.bias False\nvit.blocks.7.attn.proj.weight False\nvit.blocks.7.attn.proj.bias False\nvit.blocks.7.norm2.weight False\nvit.blocks.7.norm2.bias False\nvit.blocks.7.mlp.fc1.weight False\nvit.blocks.7.mlp.fc1.bias False\nvit.blocks.7.mlp.fc2.weight False\nvit.blocks.7.mlp.fc2.bias False\nvit.blocks.8.norm1.weight False\nvit.blocks.8.norm1.bias False\nvit.blocks.8.attn.qkv.weight False\nvit.blocks.8.attn.qkv.bias False\nvit.blocks.8.attn.proj.weight False\nvit.blocks.8.attn.proj.bias False\nvit.blocks.8.norm2.weight False\nvit.blocks.8.norm2.bias False\nvit.blocks.8.mlp.fc1.weight False\nvit.blocks.8.mlp.fc1.bias False\nvit.blocks.8.mlp.fc2.weight False\nvit.blocks.8.mlp.fc2.bias False\nvit.blocks.9.norm1.weight False\nvit.blocks.9.norm1.bias False\nvit.blocks.9.attn.qkv.weight False\nvit.blocks.9.attn.qkv.bias False\nvit.blocks.9.attn.proj.weight False\nvit.blocks.9.attn.proj.bias False\nvit.blocks.9.norm2.weight False\nvit.blocks.9.norm2.bias False\nvit.blocks.9.mlp.fc1.weight False\nvit.blocks.9.mlp.fc1.bias False\nvit.blocks.9.mlp.fc2.weight False\nvit.blocks.9.mlp.fc2.bias False\nvit.blocks.10.norm1.weight False\nvit.blocks.10.norm1.bias False\nvit.blocks.10.attn.qkv.weight False\nvit.blocks.10.attn.qkv.bias False\nvit.blocks.10.attn.proj.weight False\nvit.blocks.10.attn.proj.bias False\nvit.blocks.10.norm2.weight False\nvit.blocks.10.norm2.bias False\nvit.blocks.10.mlp.fc1.weight False\nvit.blocks.10.mlp.fc1.bias False\nvit.blocks.10.mlp.fc2.weight False\nvit.blocks.10.mlp.fc2.bias False\nvit.blocks.11.norm1.weight False\nvit.blocks.11.norm1.bias False\nvit.blocks.11.attn.qkv.weight False\nvit.blocks.11.attn.qkv.bias False\nvit.blocks.11.attn.proj.weight False\nvit.blocks.11.attn.proj.bias False\nvit.blocks.11.norm2.weight False\nvit.blocks.11.norm2.bias False\nvit.blocks.11.mlp.fc1.weight False\nvit.blocks.11.mlp.fc1.bias False\nvit.blocks.11.mlp.fc2.weight False\nvit.blocks.11.mlp.fc2.bias False\nvit.norm.weight False\nvit.norm.bias False\nvit.head.weight True\nvit.head.bias True\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/3864937168.py:11: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()\n/tmp/ipykernel_36/3864937168.py:23: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/3], Batch [10/27], Loss: 2.0335\nEpoch [1/3], Batch [20/27], Loss: 1.7466\nEpoch [1/3] completed in 14.23 seconds. Loss: 2.0524, Accuracy: 28.98%\nEpoch [2/3], Batch [10/27], Loss: 1.2980\nEpoch [2/3], Batch [20/27], Loss: 0.9211\nEpoch [2/3] completed in 14.42 seconds. Loss: 1.1251, Accuracy: 71.44%\nEpoch [3/3], Batch [10/27], Loss: 0.6090\nEpoch [3/3], Batch [20/27], Loss: 0.4847\nEpoch [3/3] completed in 14.30 seconds. Loss: 0.6199, Accuracy: 88.14%\nModel fine-tuned and saved successfully.\n","output_type":"stream"}]},{"cell_type":"code","source":"import shutil\nshutil.make_archive(\"fine_tuned_vit_pacs\", 'zip', \"/kaggle/working/fine_tuned_vit_pacs\")","metadata":{"execution":{"iopub.status.busy":"2024-09-29T20:56:36.456875Z","iopub.execute_input":"2024-09-29T20:56:36.457693Z","iopub.status.idle":"2024-09-29T20:56:36.464133Z","shell.execute_reply.started":"2024-09-29T20:56:36.457653Z","shell.execute_reply":"2024-09-29T20:56:36.463225Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/fine_tuned_vit_pacs.zip'"},"metadata":{}}]},{"cell_type":"code","source":"import torch\n\n\ndef evaluate_model(model, data_loaders, device):\n    model.eval()  \n    total_accuracy = {}\n    \n    with torch.no_grad(): \n        for domain, loader in data_loaders.items():\n            correct = 0\n            total = 0\n            \n            for images, labels in loader:\n                images, labels = images.to(device), labels.to(device)  \n                outputs = model(images) \n                _, predicted = torch.max(outputs.data, 1) \n                \n                total += labels.size(0) \n                correct += (predicted == labels).sum().item()  \n            \n            accuracy = 100 * correct / total  \n            total_accuracy[domain] = accuracy \n            print(f\"Accuracy on {domain}: {accuracy:.2f}%\")  \n    \n    return total_accuracy \n\ndef main():\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  \n\n    pacs_loaders = get_pacs_loaders(batch_size=64) \n    num_classes = 7  \n    model = load_vit_model(num_classes, device)\n    model.load_state_dict(torch.load('fine_tuned_vit_pacs.pth')) \n    model.to(device) \n\n    accuracy = evaluate_model(model, pacs_loaders, device)\n    print(\"Final Accuracies on PACS Domains:\", accuracy)\n\nmain()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-29T20:58:16.048677Z","iopub.execute_input":"2024-09-29T20:58:16.049392Z","iopub.status.idle":"2024-09-29T20:59:44.777719Z","shell.execute_reply.started":"2024-09-29T20:58:16.049351Z","shell.execute_reply":"2024-09-29T20:59:44.776695Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Entered get_pacs_loaders function\nNumber of photos: 1670\nNumber of art paintings: 2048\nNumber of cartoons: 2344\nNumber of sketches: 3929\nSample from Photos:\nImage batch shape: torch.Size([64, 3, 224, 224])\nLabel batch shape: torch.Size([64])\nSample from Art_paintings:\nImage batch shape: torch.Size([64, 3, 224, 224])\nLabel batch shape: torch.Size([64])\nSample from Cartoons:\nImage batch shape: torch.Size([64, 3, 224, 224])\nLabel batch shape: torch.Size([64])\nSample from Sketches:\nImage batch shape: torch.Size([64, 3, 224, 224])\nLabel batch shape: torch.Size([64])\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/2478311692.py:40: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('fine_tuned_vit_pacs.pth'))  # Make sure the path is correct\n","output_type":"stream"},{"name":"stdout","text":"Accuracy on Photos: 92.34%\nAccuracy on Art_paintings: 49.90%\nAccuracy on Cartoons: 29.31%\nAccuracy on Sketches: 20.64%\nFinal Accuracies on PACS Domains: {'Photos': 92.33532934131736, 'Art_paintings': 49.90234375, 'Cartoons': 29.308873720136518, 'Sketches': 20.64138457622805}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"SVNHH","metadata":{}},{"cell_type":"code","source":"import torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nfrom torch.utils.data import DataLoader\n\ndef get_svhn_data(batch_size=128):\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)), \n        transforms.ToTensor(),\n        # transforms.Normalize((0.5,), (0.5,))\n    ])\n\n    train_dataset = datasets.SVHN(root='./datamodule', download=True, split='train', transform=transform)\n    test_dataset = datasets.SVHN(root='./datamodule', download=True, split='test', transform=transform)\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n    return train_loader, test_loader\n\n# train_loader, test_loader = get_svhn_data(batch_size=128)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-29T21:12:40.577930Z","iopub.execute_input":"2024-09-29T21:12:40.578647Z","iopub.status.idle":"2024-09-29T21:12:40.586436Z","shell.execute_reply.started":"2024-09-29T21:12:40.578606Z","shell.execute_reply":"2024-09-29T21:12:40.585464Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"import timm\nimport torch\nimport torch.nn as nn\n\nclass ViTClassifier(nn.Module):\n    def __init__(self, num_classes, pretrained=True, finetune=\"classifier\"):\n        super(ViTClassifier, self).__init__()\n        self.vit = timm.create_model('vit_base_patch16_224', pretrained=pretrained)\n        self.vit.head = nn.Linear(self.vit.head.in_features, num_classes)\n\n        if finetune == \"classifier\":\n            # Freezing the backbone\n            for param in self.vit.parameters():\n                param.requires_grad = False\n            # Unfreezing the classifier\n            for param in self.vit.head.parameters():\n                param.requires_grad = True\n\n    def forward(self, x):\n        return self.vit(x)\n\ndef load_vit_model(num_classes, device):\n    model = ViTClassifier(num_classes)\n    model = model.to(device)\n    return model\n\n# Example usage (to be run in another cell if needed):\n# num_classes = 7  # Adjust based on your dataset\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# model = load_vit_model(num_classes, device)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-29T21:12:43.694733Z","iopub.execute_input":"2024-09-29T21:12:43.695105Z","iopub.status.idle":"2024-09-29T21:12:43.703687Z","shell.execute_reply.started":"2024-09-29T21:12:43.695071Z","shell.execute_reply":"2024-09-29T21:12:43.702672Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom timm import create_model\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nfrom torch.cuda.amp import autocast, GradScaler \n\n# Function to get SVHN data\ndef get_svhn_data(batch_size=128):\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)), \n        transforms.ToTensor(),\n    ])\n\n    train_dataset = datasets.SVHN(root='./datamodule', download=True, split='train', transform=transform)\n    test_dataset = datasets.SVHN(root='./datamodule', download=True, split='test', transform=transform)\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n    return train_loader, test_loader\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ndef fine_tune_vit(num_epochs=3, learning_rate=1e-4, batch_size=32):\n    \"\"\"Fine-tune a pre-trained ViT model on the SVHN dataset.\"\"\"\n    \n    train_loader, test_loader = get_svhn_data(batch_size=batch_size)\n    model = create_model('vit_base_patch16_224', pretrained=True, num_classes=10)  # SVHN has 10 classes\n    model = model.to(device)\n    \n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    scaler = GradScaler()\n\n    model.train()\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        for i, data in enumerate(train_loader, 0):\n            inputs, labels = data\n            inputs, labels = inputs.to(device), labels.to(device)\n            \n            optimizer.zero_grad()\n            with autocast():\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n            \n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n\n            running_loss += loss.item()\n            if i % 100 == 99:\n                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {running_loss / 100:.4f}')\n                running_loss = 0.0\n\n    torch.save(model.state_dict(), 'fine_tuned_vit_svhn.pth')\n    print(\"Model fine-tuned and saved successfully.\")\n\nfine_tune_vit(batch_size=64)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-26T19:02:37.952211Z","iopub.execute_input":"2024-09-26T19:02:37.952595Z","iopub.status.idle":"2024-09-26T19:35:39.128370Z","shell.execute_reply.started":"2024-09-26T19:02:37.952558Z","shell.execute_reply":"2024-09-26T19:35:39.127255Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Downloading http://ufldl.stanford.edu/housenumbers/train_32x32.mat to ./datamodule/train_32x32.mat\n100%|███████████████████████| 182040794/182040794 [00:05<00:00, 30664021.04it/s]\nDownloading http://ufldl.stanford.edu/housenumbers/test_32x32.mat to ./datamodule/test_32x32.mat\n100%|█████████████████████████| 64275384/64275384 [00:04<00:00, 15330836.16it/s]\nmodel.safetensors: 100%|██████████████████████| 346M/346M [00:01<00:00, 243MB/s]\n/kaggle/working/Fine_Tune.py:27: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()\n/kaggle/working/Fine_Tune.py:41: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\nEpoch [1/3], Step [100/1145], Loss: 2.2939\nEpoch [1/3], Step [200/1145], Loss: 2.2622\nEpoch [1/3], Step [300/1145], Loss: 2.2500\nEpoch [1/3], Step [400/1145], Loss: 2.2515\nEpoch [1/3], Step [500/1145], Loss: 2.2586\nEpoch [1/3], Step [600/1145], Loss: 2.2602\nEpoch [1/3], Step [700/1145], Loss: 2.2540\nEpoch [1/3], Step [800/1145], Loss: 2.2489\nEpoch [1/3], Step [900/1145], Loss: 2.2911\nEpoch [1/3], Step [1000/1145], Loss: 2.2499\nEpoch [1/3], Step [1100/1145], Loss: 2.2495\nEpoch [2/3], Step [100/1145], Loss: 2.2409\nEpoch [2/3], Step [200/1145], Loss: 2.1018\nEpoch [2/3], Step [300/1145], Loss: 1.5624\nEpoch [2/3], Step [400/1145], Loss: 1.0651\nEpoch [2/3], Step [500/1145], Loss: 0.7480\nEpoch [2/3], Step [600/1145], Loss: 0.5527\nEpoch [2/3], Step [700/1145], Loss: 0.4764\nEpoch [2/3], Step [800/1145], Loss: 0.4029\nEpoch [2/3], Step [900/1145], Loss: 0.3853\nEpoch [2/3], Step [1000/1145], Loss: 0.3462\nEpoch [2/3], Step [1100/1145], Loss: 0.3337\nEpoch [3/3], Step [100/1145], Loss: 0.3044\nEpoch [3/3], Step [200/1145], Loss: 0.2685\nEpoch [3/3], Step [300/1145], Loss: 0.2766\nEpoch [3/3], Step [400/1145], Loss: 0.2851\nEpoch [3/3], Step [500/1145], Loss: 0.2880\nEpoch [3/3], Step [600/1145], Loss: 0.2748\nEpoch [3/3], Step [700/1145], Loss: 0.2701\nEpoch [3/3], Step [800/1145], Loss: 0.2448\nEpoch [3/3], Step [900/1145], Loss: 0.2500\nEpoch [3/3], Step [1000/1145], Loss: 0.2398\nEpoch [3/3], Step [1100/1145], Loss: 0.2364\nModel fine-tuned and saved successfully.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom timm import create_model\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ndef inference_vit(batch_size=32):\n    \"\"\"Run inference on the SVHN test dataset using the fine-tuned ViT model.\"\"\"\n    \n    _, test_loader = get_svhn_data(batch_size=batch_size)\n\n    model = create_model('vit_base_patch16_224', pretrained=False, num_classes=10)  # 10 classes\n    model.load_state_dict(torch.load('fine_tuned_vit_svhn.pth')) \n    model = model.to(device)\n    model.eval()\n\n    criterion = nn.CrossEntropyLoss()\n\n    correct = 0\n    total = 0\n    test_loss = 0.0\n\n    with torch.no_grad():\n        for data in test_loader:\n            inputs, labels = data\n            inputs, labels = inputs.to(device), labels.to(device)\n            \n            outputs = model(inputs)\n            _, predicted = torch.max(outputs.data, 1)\n            \n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            \n            loss = criterion(outputs, labels)\n            test_loss += loss.item()\n    \n    accuracy = 100 * correct / total\n    avg_test_loss = test_loss / len(test_loader)\n    print(f'Test Accuracy: {accuracy:.2f}%')\n    print(f'Average Test Loss: {avg_test_loss:.4f}')\n\ninference_vit(batch_size=32) ","metadata":{"execution":{"iopub.status.busy":"2024-09-26T19:36:39.379339Z","iopub.execute_input":"2024-09-26T19:36:39.380193Z","iopub.status.idle":"2024-09-26T19:41:40.852212Z","shell.execute_reply.started":"2024-09-26T19:36:39.380151Z","shell.execute_reply":"2024-09-26T19:41:40.851254Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Using downloaded and verified file: ./datamodule/train_32x32.mat\nUsing downloaded and verified file: ./datamodule/test_32x32.mat\n/kaggle/working/inference.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('fine_tuned_vit_svhn.pth'))  # Load fine-tuned weights\nTest Accuracy: 93.44%\nAverage Test Loss: 0.2232\n","output_type":"stream"}]}]}